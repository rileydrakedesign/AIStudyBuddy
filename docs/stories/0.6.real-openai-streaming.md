# Story 0.6: Real OpenAI Streaming Implementation - Brownfield Addition

## Status

**READY FOR IMPLEMENTATION** (Revised after Architecture Review)

---

## Executive Summary

**What This Is**: Replace simulated character-by-character streaming with real token streaming from OpenAI.

**Current Behavior**:
- Backend waits for COMPLETE response (3-5 seconds)
- Frontend simulates typing with `setInterval` (2ms per character)
- Visual effect: `"H" â†’ "He" â†’ "Hel" â†’ "Hell" â†’ "Hello"`

**New Behavior**:
- Backend streams tokens as OpenAI generates them (first token in 1-2 seconds)
- Frontend displays tokens in real-time via WebSocket
- Visual effect: IDENTICAL progressive text, but with real tokens

**Implementation Scope**:
- **Remove**: 13 lines of `setInterval` simulation
- **Add**: WebSocket listeners (3 events), Python async streaming, Node SSE proxy
- **Preserve**: ALL existing features (citations, retry, reactions, document chat, etc.)
- **Fix**: 2 timing bugs (MongoDB save, free tier counter)

**Complexity**: Medium (3-layer change: Python â†’ Node â†’ Frontend)

**Risk**: Medium-High (touches critical chat path, requires careful testing)

**Why Not Over-Engineered**: Direct 1:1 replacement of simulated streaming with real streaming using existing WebSocket infrastructure. No new features, no architectural changes, no UI redesign.

---

## Story

**As a** student user,
**I want** real-time streaming chat responses directly from OpenAI via WebSocket instead of simulated character-by-character display,
**so that** I can see responses as they're being generated with lower perceived latency and a more responsive user experience.

---

## Story Context

**Existing System Integration:**

- **Current Implementation**:
  - Frontend receives full response from backend, then simulates streaming with `setInterval` displaying one character every 2ms (`frontend/src/pages/Chat.tsx:487-500`)
  - Node backend makes blocking POST request to Python FastAPI service (`backend/src/controllers/chat_controllers.ts:261-267`)
  - Python service uses LangChain `ChatOpenAI` with synchronous completion calls (`backend/python_scripts/semantic_search.py`)

- **Existing WebSocket Infrastructure** (already implemented):
  - Socket.IO server with JWT authentication (`backend/src/utils/socket_server.ts`)
  - Frontend socket client with singleton pattern (`frontend/src/helpers/socketClient.ts`)
  - Active events: `document-ready`, `processing-status` for document ingestion
  - User-specific rooms: `socket.join(uid)` at socket_server.ts:91

- **Technology**:
  - Python: FastAPI + LangChain + OpenAI SDK
  - Node: Express + Socket.IO 4.6.0
  - Frontend: React 18.3.1 + Socket.IO client 4.6.0

- **Follows pattern**:
  - Existing WebSocket connection for document processing status (reuse this infrastructure)
  - Existing chat message flow through `generateChatCompletion` controller
  - Architecture requirement (CLAUDE.md line 68): "Renders streaming tokens over **WebSockets**"

- **Touch points**:
  - Python: `semantic_search.py` (OpenAI LLM invocation), `semantic_service.py` (existing StreamingResponse)
  - Node: `chat_controllers.ts:generateChatCompletion`, `socket_server.ts` (Socket.IO instance)
  - Frontend: `Chat.tsx` (handleSubmit, message state management), `socketClient.ts`

---

## Acceptance Criteria

### Functional Requirements

1. **Python Service - OpenAI Token Streaming**
   - Python service streams tokens from OpenAI using LangChain async streaming callbacks
   - Each token yielded as Server-Sent Event (SSE) from Python service
   - Citations and chunk references included at end of stream (after `[DONE]` marker)
   - Streaming works for all query routes: `general_qa`, `follow_up`, `quote_finding`, `generate_study_guide`
   - Error handling: Stream errors caught and sent as error event
   - New endpoint: `/api/v1/semantic_search_stream` (keep existing `/api/v1/semantic_search` for backward compatibility)

2. **Node Backend - WebSocket Proxy**
   - Node backend proxies streaming responses from Python SSE to frontend via **WebSocket events**
   - Reuses existing Socket.IO infrastructure (`backend/src/utils/socket_server.ts`)
   - New WebSocket events emitted to user's room:
     - `chat-stream-token`: Individual token/chunk from OpenAI
     - `chat-stream-complete`: Stream finished with citations and chunk references
     - `chat-stream-error`: Stream failed with error message
   - Maintains existing chat session persistence (accumulate full response server-side, save to MongoDB after stream completes)
   - Rate limiting and auth checks happen before streaming starts
   - Supports retry mechanism by clearing last assistant message before re-streaming

3. **Frontend - Real-time WebSocket Display**
   - Remove simulated streaming logic (`typeIntervalRef` setInterval code at line 487-500)
   - Listen for WebSocket events to display tokens as they arrive
   - Accumulate partial response in state (`partialAssistantMessage`)
   - Display typing indicator while waiting for first token
   - Handle stream completion: finalize message, add to chat history, clean up listeners
   - Handle stream errors: show error toast, allow retry, clean up listeners
   - Cleanup WebSocket listeners on component unmount (prevent memory leaks)

4. **Backwards Compatibility**
   - Existing chat functionality (citations, chunk references, reactions) works unchanged
   - Document chat with page-level citations continues to work
   - Chat session persistence (MongoDB) maintains existing schema
   - Free tier rate limiting (25 chats/month) enforced only after successful stream completion
   - Existing WebSocket events (`document-ready`, `processing-status`) unaffected

### Integration Requirements

5. Existing `generateChatCompletion` controller flow preserved (auth, session creation, class/doc assignment)
6. Python RAG pipeline (retrieval, routing, citation generation) continues to work unchanged
7. Frontend message state management (`chatMessages`, `partialAssistantMessage`) maintains current behavior
8. WebSocket connection initialization (`initializeSocket`) in `Chat.tsx` remains functional for all events
9. Redis integration: Token reservation, streaming state management, cleanup for interrupted streams

### Quality Requirements

10. First token arrives within 1-2 seconds (vs. current 3-5 seconds for full response)
11. Streaming has <5% failure rate (fallback to error message with retry option)
12. No memory leaks from unclosed streams (verify cleanup on component unmount with React StrictMode)
13. Manual testing with 10+ diverse queries confirms streaming works across all routes
14. MongoDB writes only after stream completes (no incremental writes during streaming)

---

## Technical Notes

### Integration Approach

**Architecture Decision**: Use WebSocket (not SSE) as specified in CLAUDE.md Guardrail #5: "Keep streaming over WebSockets intact for chat UX"

**Why WebSocket over SSE**:
- Existing Socket.IO infrastructure already implemented with JWT auth
- Avoids URL length limits of SSE GET requests (current flow uses POST with large chat history)
- Better handling of Heroku timeout constraints (WebSocket bypasses HTTP timeout after upgrade)
- Bidirectional communication allows client to pause/cancel streams
- Authentication already solved (JWT middleware in socket_server.ts:36-75)

---

### Python Service Changes

**File**: `backend/python_scripts/semantic_search.py`

**Add New Streaming Endpoint**:

```python
# New async streaming function
async def stream_semantic_search(
    user_id: str,
    class_name: str | None,
    doc_id: str | None,
    user_query: str,
    chat_history: list,
    source: str
) -> StreamingResponse:
    """
    Async streaming version of process_semantic_search.
    Yields tokens in real-time via SSE format.
    """

    # 1. Token reservation (before streaming starts)
    estimated_tokens = estimate_tokens(user_query, chat_history)
    if not try_acquire_tokens(user_id, estimated_tokens):
        raise HTTPException(status_code=429, detail="Rate limit exceeded")

    # 2. Perform retrieval (synchronous - same as existing)
    route = detect_route(user_query, chat_history)
    cfg = ROUTE_CONFIG[route]

    query_vec = embedding_model.embed_query(user_query)
    similarity_results = perform_semantic_search(
        query_vec,
        filters=build_filters(user_id, class_name, doc_id),
        limit=cfg["k"],
        numCandidates=cfg["numCandidates"]
    )

    chunks = list(similarity_results)
    context = build_context_from_chunks(chunks)

    # 3. Setup streaming callback
    callback = TokenStreamingCallback()
    llm = ChatOpenAI(
        model=config.OPENAI_CHAT_MODEL,
        temperature=cfg["temperature"],
        streaming=True,
        callbacks=[callback]
    )

    # 4. Build prompt (same as existing)
    prompt_template = get_prompt_for_route(route)
    chain = prompt_template | llm | StrOutputParser()

    # 5. Streaming generator function
    async def token_generator():
        try:
            # Start LLM streaming in background
            task = asyncio.create_task(
                chain.ainvoke({
                    "input": user_query,
                    "chat_history": format_chat_history(chat_history),
                    "context": context
                })
            )

            # Yield tokens as they arrive
            while True:
                try:
                    event = await asyncio.wait_for(callback.queue.get(), timeout=1.0)

                    if event["type"] == "done":
                        # Compute citations (same as existing _renumber_citations)
                        citations = generate_citations(chunks)
                        chunk_refs = generate_chunk_references(chunks)

                        # Send final metadata
                        yield f"data: {json.dumps({'type': 'done', 'citations': citations, 'chunkReferences': chunk_refs})}\n\n"
                        break
                    else:
                        # Yield token
                        yield f"data: {json.dumps(event)}\n\n"

                except asyncio.TimeoutError:
                    # Keepalive to prevent Heroku timeout
                    yield f"data: {json.dumps({'type': 'keepalive'})}\n\n"

            await task  # Wait for completion

        except Exception as e:
            log.error(f"Streaming error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(token_generator(), media_type="text/event-stream")


# Async callback handler with queue pattern
class TokenStreamingCallback(AsyncCallbackHandler):
    """
    LangChain async callback handler for token streaming.
    Uses asyncio.Queue to bridge between callback and generator.
    """

    def __init__(self):
        self.queue = asyncio.Queue()

    async def on_llm_new_token(self, token: str, **kwargs):
        """Called for each new token from OpenAI."""
        await self.queue.put({"type": "token", "content": token})

    async def on_llm_end(self, response, **kwargs):
        """Called when LLM completes."""
        await self.queue.put({"type": "done"})

    async def on_llm_error(self, error: Exception, **kwargs):
        """Called on LLM errors."""
        await self.queue.put({"type": "error", "message": str(error)})
```

**Add to FastAPI Router** (`semantic_service.py`):

```python
@app.post("/api/v1/semantic_search_stream")
async def semantic_search_stream_endpoint(
    user_id: str = Body(...),
    class_name: str | None = Body(None),
    doc_id: str | None = Body(None),
    user_query: str = Body(...),
    chat_history: list = Body([]),
    source: str = Body("main_app")
):
    """New streaming endpoint (async)."""
    return await stream_semantic_search(
        user_id, class_name, doc_id, user_query, chat_history, source
    )

# Keep existing endpoint for backward compatibility
@app.post("/api/v1/semantic_search")
def semantic_search_endpoint(...):
    """Existing synchronous endpoint (unchanged)."""
    return process_semantic_search(...)
```

---

### Node Backend Changes

**File**: `backend/src/controllers/chat_controllers.ts`

**Modify `generateChatCompletion` to Proxy Streaming**:

```typescript
import { io } from '../utils/socket_server.js';

export const generateChatCompletion = async (req, res, next) => {
  const { message, class_name, docId, chatSessionId, ephemeral, retry } = req.body;
  const classNameForPython = class_name && class_name !== "null" ? class_name : null;
  const docIdForPython = docId && docId !== "null" ? docId : null;

  try {
    /* ---------- AUTH ---------- */
    const currentUser = await User.findById(res.locals.jwtData.id);
    if (!currentUser) {
      return res.status(401).json({ message: "User not registered or token malfunctioned" });
    }

    /* ---------- FREE-TIER LIMIT (deferred until stream completes) ---------- */
    const isFreeUser = currentUser.plan === "free";
    let shouldIncrementCount = false;

    if (isFreeUser) {
      const now = new Date();
      if (
        !currentUser.chatRequestResetAt ||
        now.getMonth() !== currentUser.chatRequestResetAt.getMonth() ||
        now.getFullYear() !== currentUser.chatRequestResetAt.getFullYear()
      ) {
        currentUser.chatRequestCount = 0;
        currentUser.chatRequestResetAt = now;
      }

      if (currentUser.chatRequestCount >= 25) {
        return res.status(403).json({
          message: "Free plan limit reached (25 chats/month). Upgrade to premium for unlimited chats.",
        });
      }

      shouldIncrementCount = true;  // Will increment after successful stream
    }

    /* ---------- SESSION BOOKKEEPING (same as existing) ---------- */
    const userId = currentUser._id;
    const sourceHeader = req.headers["x-source"];
    const source = sourceHeader === "chrome_extension" ? "chrome_extension" : "main_app";

    let chatSession;
    // ... (existing session creation logic unchanged) ...

    if (!retry) {
      chatSession.messages.push({
        content: message,
        role: "user",
        citation: null,
        chunkReferences: [],
      });
    }

    /* ---------- PREPARE PYTHON PAYLOAD ---------- */
    const chats = chatSession.messages.map(({ role, content, citation, chunkReferences }) => ({
      role,
      content,
      citation,
      chunkReferences,
    }));

    const pythonApiUrl = process.env.PYTHON_API_URL;
    const streamEndpoint = `${pythonApiUrl}/api/v1/semantic_search_stream`;

    const requestData = {
      user_id: userId.toString(),
      class_name: chatSession.assignedClass || "null",
      doc_id: chatSession.assignedDocument || "null",
      user_query: message,
      chat_history: chats,
      source,
    };

    /* ---------- STREAM FROM PYTHON VIA WEBSOCKET ---------- */
    const userRoom = userId.toString();
    let fullResponse = "";
    let citations = null;
    let chunkReferences = [];
    let streamError = null;

    try {
      const pythonStream = await axios.post(streamEndpoint, requestData, {
        responseType: 'stream',
        headers: { 'X-Request-ID': (req as any).id }
      });

      pythonStream.data.on('data', (chunk: Buffer) => {
        const text = chunk.toString();
        const lines = text.split('\n').filter(line => line.trim().startsWith('data:'));

        lines.forEach(line => {
          try {
            const jsonStr = line.replace('data:', '').trim();
            if (!jsonStr) return;

            const event = JSON.parse(jsonStr);

            if (event.type === 'token') {
              // Emit token to user's WebSocket room
              io.to(userRoom).emit('chat-stream-token', {
                sessionId: chatSession._id.toString(),
                token: event.content
              });
              fullResponse += event.content;

            } else if (event.type === 'done') {
              // Store metadata for final persistence
              citations = event.citations;
              chunkReferences = event.chunkReferences;

            } else if (event.type === 'keepalive') {
              // Ignore keepalive events

            } else if (event.type === 'error') {
              streamError = event.message;
            }
          } catch (parseError) {
            (req as any).log.warn({ err: parseError, line }, "Failed to parse SSE event");
          }
        });
      });

      pythonStream.data.on('end', async () => {
        if (streamError) {
          // Emit error to user
          io.to(userRoom).emit('chat-stream-error', {
            sessionId: chatSession._id.toString(),
            error: streamError
          });
          return res.status(500).json({ message: streamError });
        }

        // Update citation text if single-document chat
        if (chatSession.assignedDocument && citations && Array.isArray(citations)) {
          try {
            let doc = await Document.findOne({ docId: chatSession.assignedDocument });
            if (!doc) doc = await Document.findById(chatSession.assignedDocument);
            if (doc) {
              citations = citations.map((cit) => ({ ...cit, text: doc.fileName }));
            }
          } catch (docError) {
            (req as any).log.warn({ err: docError }, "Error fetching document for citation");
          }
        }

        // Handle retry vs new message
        if (retry === true) {
          const lastIdx = chatSession.messages.length - 2;
          if (lastIdx >= 0 && chatSession.messages[lastIdx].role === "assistant") {
            const prevMsg = chatSession.messages[lastIdx];
            if (!prevMsg.versions) prevMsg.versions = [prevMsg.content];
            prevMsg.versions.push(fullResponse);
            prevMsg.currentVersion = prevMsg.versions.length - 1;
            prevMsg.content = fullResponse;
            prevMsg.citation = citations;
            prevMsg.chunkReferences = chunkReferences;
          } else {
            chatSession.messages.push({
              content: fullResponse,
              role: "assistant",
              citation: citations,
              chunkReferences: chunkReferences,
            });
          }
        } else {
          chatSession.messages.push({
            content: fullResponse,
            role: "assistant",
            citation: citations,
            chunkReferences: chunkReferences,
          });
        }

        await chatSession.save();

        // Increment free tier count only after successful completion
        if (shouldIncrementCount) {
          currentUser.chatRequestCount += 1;
          await currentUser.save();
        }

        // Emit completion to user
        io.to(userRoom).emit('chat-stream-complete', {
          sessionId: chatSession._id.toString(),
          citations: citations,
          chunkReferences: chunkReferences
        });

        // HTTP response (for non-WebSocket clients)
        return res.status(200).json({
          chatSessionId: chatSession._id,
          messages: chatSession.messages,
          assignedClass: chatSession.assignedClass,
          assignedDocument: chatSession.assignedDocument,
        });
      });

      pythonStream.data.on('error', (error) => {
        (req as any).log.error({ err: error }, "Python stream error");
        io.to(userRoom).emit('chat-stream-error', {
          sessionId: chatSession._id.toString(),
          error: error.message
        });
        return res.status(500).json({ message: "Stream failed" });
      });

    } catch (error) {
      (req as any).log.error({ err: error }, "Failed to initiate stream");
      io.to(userRoom).emit('chat-stream-error', {
        sessionId: chatSession._id?.toString(),
        error: error.message
      });
      return res.status(500).json({ message: "Failed to start stream" });
    }

  } catch (error) {
    (req as any).log.error(error);
    return res.status(500).json({ message: "Something went wrong" });
  }
};
```

---

### Frontend Changes

**File**: `frontend/src/pages/Chat.tsx`

**Replace Simulated Streaming with WebSocket Consumption**:

```typescript
// Add to top of file
import { initializeSocket } from "../helpers/socketClient";

// ... existing imports and component setup ...

const Chat = () => {
  // ... existing state ...

  // Add cleanup ref for WebSocket listeners
  const streamListenersRef = useRef<{
    token: (data: any) => void;
    complete: (data: any) => void;
    error: (data: any) => void;
  } | null>(null);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (streamListenersRef.current) {
        const socket = initializeSocket();
        socket.off('chat-stream-token', streamListenersRef.current.token);
        socket.off('chat-stream-complete', streamListenersRef.current.complete);
        socket.off('chat-stream-error', streamListenersRef.current.error);
      }
    };
  }, []);

  const handleSubmit = async () => {
    const content = inputRef.current?.value.trim();
    if (!content || isGenerating) return;

    const newMessage = { role: "user" as const, content };
    const updatedMessages = [...chatMessages, newMessage];
    setChatMessages(updatedMessages);
    setIsGenerating(true);
    setPartialAssistantMessage("");

    if (inputRef.current) {
      inputRef.current.value = "";
    }

    // Setup WebSocket listeners
    const socket = initializeSocket();

    const handleToken = (data: { sessionId: string; token: string }) => {
      if (data.sessionId === currentChatSessionId) {
        setPartialAssistantMessage(prev => prev + data.token);
      }
    };

    const handleComplete = (data: {
      sessionId: string;
      citations: any[];
      chunkReferences: any[];
    }) => {
      if (data.sessionId === currentChatSessionId) {
        // Finalize message
        const finalMessage = {
          role: "assistant" as const,
          content: partialAssistantMessage,
          citation: data.citations,
          chunkReferences: data.chunkReferences
        };

        setChatMessages(prev => [...prev, finalMessage]);
        setPartialAssistantMessage("");
        setIsGenerating(false);

        // Cleanup listeners
        socket.off('chat-stream-token', handleToken);
        socket.off('chat-stream-complete', handleComplete);
        socket.off('chat-stream-error', handleError);
        streamListenersRef.current = null;
      }
    };

    const handleError = (data: { sessionId: string; error: string }) => {
      if (data.sessionId === currentChatSessionId) {
        toast.error("Stream interrupted. Please retry.");
        setIsGenerating(false);

        // Cleanup listeners
        socket.off('chat-stream-token', handleToken);
        socket.off('chat-stream-complete', handleComplete);
        socket.off('chat-stream-error', handleError);
        streamListenersRef.current = null;
      }
    };

    // Store listeners for cleanup
    streamListenersRef.current = { token: handleToken, complete: handleComplete, error: handleError };

    socket.on('chat-stream-token', handleToken);
    socket.on('chat-stream-complete', handleComplete);
    socket.on('chat-stream-error', handleError);

    // Trigger streaming via HTTP POST
    try {
      await sendChatRequest(content, currentChatSessionId, selectedClass, activeDocId);
      // Response will come via WebSocket events, not HTTP response
    } catch (error: any) {
      console.error("Error in handleSubmit:", error);

      // Cleanup listeners on error
      socket.off('chat-stream-token', handleToken);
      socket.off('chat-stream-complete', handleComplete);
      socket.off('chat-stream-error', handleError);
      streamListenersRef.current = null;

      if (error.response && error.response.status === 403) {
        toast.error(
          error.response.data.message || "Monthly chat limit reached for the free plan"
        );
      } else {
        toast.error("Failed to send message");
      }
      setIsGenerating(false);
    }
  };

  // REMOVE lines 483-500 (old simulated streaming with setInterval)
  // DELETE the entire typeIntervalRef and setInterval logic

  // ... rest of component unchanged ...
};
```

---

### Redis Integration Specification

**Purpose**: Track streaming state for fault tolerance and cleanup

**Key Patterns**:

```python
# In semantic_search.py

async def stream_semantic_search(...):
    # Create stream state key
    stream_key = f"chat:stream:{user_id}:{session_id}:{int(time.time())}"

    # Store stream metadata
    r.setex(stream_key, 300, json.dumps({
        "user_id": user_id,
        "session_id": session_id,
        "started_at": datetime.now().isoformat(),
        "status": "in_progress"
    }))

    try:
        async def token_generator():
            # ... streaming logic ...

            # Update stream state periodically
            r.expire(stream_key, 300)  # Refresh TTL

            yield tokens...

        # On completion
        r.delete(stream_key)

    except Exception as e:
        # Mark as failed
        r.setex(stream_key, 60, json.dumps({
            "status": "failed",
            "error": str(e)
        }))
        raise
```

**Cleanup Strategy**:
- Stream keys auto-expire after 5 minutes (TTL: 300 seconds)
- Successful completions delete key immediately
- Failed streams kept for 1 minute for debugging
- No manual cleanup required (Redis handles expiration)

---

### Error Recovery Specification

**Stream Interruption Scenarios**:

1. **User Disconnects Mid-Stream**:
   - WebSocket disconnect detected by Node backend
   - Python stream continues (no cancellation mechanism needed)
   - MongoDB saves complete response when Python finishes
   - User reconnects â†’ sees complete message on next page load

2. **Python Service Crashes**:
   - Node backend detects stream end without `[DONE]` event
   - Emits `chat-stream-error` to user
   - Frontend displays "Stream interrupted" toast with retry button
   - MongoDB does not save partial response

3. **Network Timeout**:
   - Python keepalive events prevent Heroku timeout (semantic_service.py:62-88)
   - WebSocket timeout: 60 seconds (configurable via `STREAM_TIMEOUT_MS`)
   - If no token received within 60s, emit error

**Retry Logic**:
- Frontend: Clear partial message, send new request with `retry: true`
- Node: Check if last message is incomplete (no `[DONE]` received)
- If retry, delete last assistant message before re-streaming
- Python: Full re-retrieval and generation (no caching)

---

### Environment Variables

**Add to `backend/.env.example`**:

```bash
# Streaming Configuration (add after line 21)
STREAM_TIMEOUT_MS=60000          # WebSocket timeout for chat streaming (60 seconds)
STREAM_KEEPALIVE_MS=10000        # Keepalive interval for long-running streams
ENABLE_STREAMING=true            # Feature flag for gradual rollout
```

**Add to `backend/python_scripts/.env.example`**:

```bash
# Streaming Configuration (add after OPENAI_TPM_LIMIT)
OPENAI_STREAMING_ENABLED=true   # Enable token streaming from OpenAI
STREAM_CHUNK_SIZE=1              # Tokens per emission (1 = real-time, 5 = batched)
STREAM_MAX_DURATION_S=120        # Max streaming duration before force-stop
```

**No changes to `frontend/.env.example`**:
- `VITE_WS_URL` already exists and will be used for streaming

---

### MongoDB Persistence Constraint

**CRITICAL**: Do NOT write partial responses to MongoDB during streaming.

**Why**:
- Prevents write amplification (1000s of writes per chat)
- Avoids partial message corruption
- Prevents MongoDB connection pool exhaustion

**Implementation**:
- Accumulate full response in Node backend memory (`fullResponse` variable)
- Only call `chatSession.save()` after `chat-stream-complete` event
- On stream error, do NOT save partial response to MongoDB
- Redis (not MongoDB) stores stream state for fault tolerance

---

### Monitoring and Observability

**Add Streaming Metrics**:

**Python** (`semantic_search.py`):
```python
# Log stream lifecycle
log.info("[STREAM] started", extra={
    "user_id": user_id,
    "session_id": session_id,
    "route": route
})

# Track first token latency
first_token_time = time.time()
log.info("[STREAM] first_token", extra={
    "latency_ms": (first_token_time - start_time) * 1000
})

# Track completion
log.info("[STREAM] completed", extra={
    "total_tokens": token_count,
    "duration_ms": (time.time() - start_time) * 1000
})
```

**Node** (`chat_controllers.ts`):
```typescript
// Use existing logger
(req as any).log.info({
  event: 'stream_started',
  userId: userId.toString(),
  sessionId: chatSession._id.toString()
});

// Track completion
(req as any).log.info({
  event: 'stream_completed',
  tokenCount: fullResponse.length,
  durationMs: Date.now() - startTime
});
```

**Heroku Metrics to Monitor**:
- `stream.started` (counter)
- `stream.completed` (counter)
- `stream.failed` (counter)
- `stream.first_token_latency_ms` (histogram)
- `stream.total_duration_ms` (histogram)

---

## Definition of Done

### Python Service
- [ ] New endpoint `/api/v1/semantic_search_stream` created with async streaming
- [ ] `TokenStreamingCallback` implemented with asyncio.Queue pattern
- [ ] Tokens yielded as SSE format: `data: {"type": "token", "content": "..."}\n\n`
- [ ] Citations and chunk references sent after `[DONE]` marker
- [ ] Keepalive events sent every 10 seconds to prevent Heroku timeout
- [ ] Existing `/api/v1/semantic_search` endpoint unchanged (backward compatibility)
- [ ] All query routes tested (general_qa, follow_up, quote_finding, generate_study_guide)
- [ ] Redis stream state keys created and cleaned up correctly

### Node Backend
- [ ] `generateChatCompletion` modified to proxy Python SSE to WebSocket
- [ ] Three WebSocket events emitted: `chat-stream-token`, `chat-stream-complete`, `chat-stream-error`
- [ ] Full response accumulated server-side in memory
- [ ] MongoDB persistence only after `chat-stream-complete` (no incremental writes)
- [ ] Free tier count incremented only after successful stream completion
- [ ] Retry logic clears last message before re-streaming
- [ ] Citation text updated for single-document chats
- [ ] Error handling for Python stream failures

### Frontend
- [ ] Simulated streaming code removed (lines 483-500 of Chat.tsx)
- [ ] WebSocket listeners added for 3 stream events
- [ ] Tokens displayed in real-time as they arrive
- [ ] `partialAssistantMessage` accumulates tokens correctly
- [ ] Finalized message added to `chatMessages` on completion
- [ ] WebSocket listeners cleaned up in `useEffect` unmount
- [ ] No duplicate listeners with React StrictMode enabled
- [ ] Error toast displayed on stream failure with retry button
- [ ] First token latency measured (target: <2 seconds)

### Integration Testing
- [ ] End-to-end test: Send query, verify tokens stream in real-time
- [ ] All 4 query routes tested (general_qa, follow_up, quote_finding, generate_study_guide)
- [ ] Citations clickable after stream completes
- [ ] Document chat with page-level citations works
- [ ] Retry mechanism tested (interrupt stream, click retry)
- [ ] Free tier limit enforced (26th chat rejected for free user)
- [ ] Concurrent users tested (2+ users streaming simultaneously)
- [ ] Memory leak test: Send 10 consecutive queries, check for listener accumulation
- [ ] React StrictMode enabled: No duplicate tokens displayed
- [ ] Error scenarios tested: Python crash, network timeout, user disconnect

### Environment and Configuration
- [ ] `STREAM_TIMEOUT_MS` added to `backend/.env.example`
- [ ] `STREAM_KEEPALIVE_MS` added to `backend/.env.example`
- [ ] `ENABLE_STREAMING` feature flag added to `backend/.env.example`
- [ ] `OPENAI_STREAMING_ENABLED` added to `backend/python_scripts/.env.example`
- [ ] `STREAM_MAX_DURATION_S` added to `backend/python_scripts/.env.example`
- [ ] Environment variables documented in DEVELOPMENT.md

### Monitoring
- [ ] Stream lifecycle logged in Python (started, first_token, completed)
- [ ] Stream events logged in Node (started, completed, failed)
- [ ] Heroku logs showing stream metrics
- [ ] First token latency tracked (<2s target met)

### Quality Checks
- [ ] No console errors or warnings in frontend
- [ ] No memory leaks detected (Chrome DevTools Memory profiler)
- [ ] Streaming success rate >95% (manual testing)
- [ ] Backward compatibility verified (existing chats load correctly)
- [ ] Code follows existing patterns (Python: snake_case, Node: camelCase, React: hooks)

---

## Risk and Compatibility Check

### Risk Assessment

**Primary Risk**: Stream interruptions leave chat in incomplete state
- **Mitigation**: Accumulate full response server-side. Only save to MongoDB after `[DONE]` event. On error, do not persist partial response.
- **Rollback**: Revert Python, Node, and Frontend changes. Simulated streaming code restored from git (commit `b397adbc`).

**Secondary Risk**: Memory leaks from unclosed WebSocket listeners
- **Mitigation**: Cleanup listeners in `useEffect` unmount. Store listener refs for cleanup. Test with React StrictMode enabled.
- **Verification**: Chrome DevTools Memory profiler after 10+ consecutive queries.

**Tertiary Risk**: Free tier users charged for failed streams
- **Mitigation**: Increment `chatRequestCount` only after successful `chat-stream-complete`. Rollback on error.

**Quaternary Risk**: Heroku dyno sleeping causes stream timeout
- **Mitigation**: Python keepalive events every 10s (already implemented). Frontend displays "Waking up server..." if response >3s.

### Compatibility Verification

- [x] No breaking changes to MongoDB schema (message schema unchanged)
- [x] No breaking changes to REST API (new streaming endpoint added, old endpoint preserved)
- [x] UI preserves existing behavior (citations, reactions, typing indicator)
- [x] Performance improvement expected (first token: 3-5s â†’ 1-2s)
- [x] WebSocket infrastructure already exists (Socket.IO server operational)
- [x] JWT authentication already implemented for WebSocket
- [x] Redis integration follows existing patterns (token reservation)

---

## Simplified Implementation Summary

### What Changes and What Stays the Same

#### Frontend (Chat.tsx) - 1 file
```typescript
// âŒ REMOVE (13 lines):
typeIntervalRef.current = setInterval(() => {
  i += 1;
  setPartialAssistantMessage(fullText.substring(0, i));  // Substring simulation
}, 2);

// âœ… ADD (WebSocket listeners):
socket.on('chat-stream-token', (data) => {
  setPartialAssistantMessage(prev => prev + data.token);  // Real token append
});
socket.on('chat-stream-complete', (data) => { /* finalize */ });
socket.on('chat-stream-error', (data) => { /* handle error */ });

// âœ… PRESERVE (100+ lines unchanged):
- All state management (partialAssistantMessage, isGenerating)
- Auto-scroll behavior
- Document-ready listeners
- Class selection
- Free tier display
- Message reactions
- Everything else
```

#### Node Backend (chat_controllers.ts) - 1 file
```typescript
// âŒ REPLACE (blocking POST):
const responseFromPython = await axios.post(semanticSearchEndpoint, requestData);

// âœ… WITH (streaming proxy):
const pythonStream = await axios.post(streamEndpoint, requestData, { responseType: 'stream' });
pythonStream.data.on('data', (chunk) => {
  // Parse SSE â†’ emit WebSocket events
  io.to(userRoom).emit('chat-stream-token', { sessionId, token });
});

// âš ï¸ FIX (timing bug):
// Move: chatSession.save() â†’ inside stream.on('end')
// Move: chatRequestCount++ â†’ inside stream.on('end')

// âœ… PRESERVE (200+ lines unchanged):
- Auth check
- Session creation
- User message push
- Citation text update
- Retry mechanism
- Everything else
```

#### Python Service (semantic_search.py, semantic_service.py) - 2 files
```python
# âœ… ADD (new async function):
async def stream_semantic_search(...) -> StreamingResponse:
    # 1. Retrieval (REUSE existing logic)
    chunks = perform_semantic_search(...)

    # 2. Stream tokens
    callback = TokenStreamingCallback()
    llm = ChatOpenAI(streaming=True, callbacks=[callback])

    async def token_generator():
        while True:
            event = await callback.queue.get()
            if event["type"] == "done":
                yield f"data: {json.dumps({'citations': ...})}\n\n"
                break
            yield f"data: {json.dumps(event)}\n\n"

# âœ… PRESERVE (1100+ lines unchanged):
- Existing process_semantic_search() function
- All helper functions (50+ functions)
- All query routes (general_qa, follow_up, quote_finding, study_guide)
- Citation generation
- Token reservation
- Error handling
- Everything else

# âœ… ADD (new endpoint):
@app.post("/api/v1/semantic_search_stream")
async def semantic_search_stream_endpoint(req: SearchRequest):
    return await stream_semantic_search(...)

# âœ… PRESERVE (existing endpoint):
@app.post("/api/v1/semantic_search")  # Unchanged for backward compatibility
```

#### WebSocket Infrastructure - 0 files changed
```typescript
// âœ… NO CHANGES (already ready):
- socket_server.ts (JWT auth, user rooms)
- socketClient.ts (singleton pattern)
```

### Files Summary
| File | Lines Changed | Type |
|------|---------------|------|
| `frontend/src/pages/Chat.tsx` | -13 deleted, +80 added | Modified |
| `backend/src/controllers/chat_controllers.ts` | +150 modified | Modified |
| `backend/python_scripts/semantic_search.py` | +250 added | Added (new function) |
| `backend/python_scripts/semantic_service.py` | +10 added | Added (new endpoint) |
| `backend/src/utils/socket_server.ts` | 0 | No change |
| `frontend/src/helpers/socketClient.ts` | 0 | No change |
| **Total** | **~490 lines** | **4 files** |

**Unchanged**: ~100 other files (all existing features preserved)

---

## Implementation Phases

### Phase 1: Python Token Streaming (3-4 hours)

1. Create `TokenStreamingCallback` class with asyncio.Queue pattern
2. Create `stream_semantic_search()` async function
3. Add `/api/v1/semantic_search_stream` FastAPI endpoint
4. Test with `curl -N http://localhost:8000/api/v1/semantic_search_stream` (verify SSE format)
5. Verify Redis stream state keys created and cleaned up

### Phase 2: Node WebSocket Proxy (3-4 hours)

1. Modify `generateChatCompletion` to call Python streaming endpoint
2. Parse SSE events from Python and emit to WebSocket
3. Accumulate full response in memory
4. Save to MongoDB only after `chat-stream-complete`
5. Test with WebSocket client tool (wscat or Postman)

### Phase 3: Frontend WebSocket Consumption (2-3 hours)

1. Remove simulated streaming code (lines 483-500)
2. Add WebSocket event listeners for 3 stream events
3. Update state management (`partialAssistantMessage` accumulation)
4. Add cleanup in `useEffect` unmount
5. Test in browser with Chrome DevTools WebSocket tab

### Phase 4: Integration Testing (2-3 hours)

1. End-to-end test with all services running locally
2. Memory leak testing (React StrictMode + 10+ consecutive queries)
3. Error scenario testing (disconnect, timeout, retry)
4. Free tier limit testing (verify count only increments on success)
5. Performance testing (measure first token latency)

**Total Estimated Time**: 10-14 hours

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-03 | 1.0 | Story created for real OpenAI streaming implementation | John (PM Agent) |
| 2025-11-03 | 2.0 | **REVISED after Architecture Review**: Switched from SSE to WebSocket, added Node WebSocket proxy spec, fixed Python streaming pattern, added Redis integration, error recovery, environment variables, expanded testing | John (PM Agent) |

---

## Previous Story Reference

**Story 0.5:** UI Polish & Fixes (formula rendering, toast notifications)

---

## Architecture Review Notes

This story was comprehensively reviewed by an architecture agent and updated to address:
- **Critical Conflict**: SSE vs WebSocket architecture mismatch â†’ Resolved by using WebSocket per CLAUDE.md
- **Missing Integration**: Node backend WebSocket emission â†’ Added complete specification
- **Incorrect Pattern**: Python callback/yield pattern â†’ Fixed with async queue pattern
- **Incomplete Redis**: Stream state management â†’ Added key patterns and cleanup
- **Missing Auth**: SSE authentication issues â†’ Resolved by using authenticated WebSocket
- **Missing Recovery**: Stream interruption handling â†’ Added comprehensive error recovery
- **Missing Config**: Environment variables â†’ Added to all 3 services
- **Memory Leaks**: Listener cleanup â†’ Added useEffect cleanup specification
- **Free Tier Accounting**: Timing clarified â†’ Only count successful streams
- **Monitoring**: Observability â†’ Added streaming metrics

---

*ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)*

*Co-Authored-By: Claude <noreply@anthropic.com>*
